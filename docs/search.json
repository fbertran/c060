[{"path":"https://fbertran.github.io/c060/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martin Sill. Author. Thomas Hielscher. Author. Manuela Zucknick. Author. Natalia Becker. Author. Frederic Bertrand. Maintainer.","code":""},{"path":"https://fbertran.github.io/c060/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Martin Sill, Thomas Hielscher, Manuela Zucknick, Natalia Becker Frederic Bertrand (2025). Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, R package version 0.4-0. doi:10.32614/CRAN.package.c060. Martin Sill, Thomas Hielscher, Natalia Becker, Manuela Zucknick (2014). c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models. Journal Statistical Software, 62(5), 1-22. doi: 10.18637/jss.v062.i05.","code":"@Manual{,   title = {Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models},   author = {Martin Sill and Thomas Hielscher and Manuela Zucknick and Natalia Becker and Frederic Bertrand},   year = {2025},   note = {R package version 0.4-0},   url = {https://github.com/fbertran/c060},   doi = {10.32614/CRAN.package.c060}, } @Article{,   title = {{c060}: Extended Inference with Lasso and Elastic-Net Regularized Cox and Generalized Linear Models},   author = {Martin Sill and Thomas Hielscher and Natalia Becker and Manuela Zucknick},   journal = {Journal of Statistical Software},   year = {2014},   volume = {62},   number = {5},   pages = {1--22},   doi = {10.18637/jss.v062.i05}, }"},{"path":[]},{"path":[]},{"path":"https://fbertran.github.io/c060/index.html","id":"maintainer-f-bertrand","dir":"","previous_headings":"","what":"Maintainer F. Bertrand","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"https://doi.org/10.32614/CRAN.package.c060 goal c060 package provide additional functions perform stability selection, model validation parameter tuning glmnet models.","code":""},{"path":"https://fbertran.github.io/c060/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"can install released version c060 CRAN : development version GitHub :","code":"install.packages(\"c060\") install.packages(\"devtools\") devtools::install_github(\"fbertran/c060\")"},{"path":[]},{"path":"https://fbertran.github.io/c060/index.html","id":"gaussian-stability-selection","dir":"","previous_headings":"","what":"Gaussian Stability Selection","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"","code":"set.seed(1234) x=matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000] %*% c(rep(2,5),rep(-2,5),rep(.1,990)) res <- stabpath(y,x,weakness=1,mc.cores=2) stabsel(res,error=0.05,type=\"pfer\") #> $stable #> integer(0) #>  #> $lambda #> [1] 2.02235 #>  #> $lpos #> [1] 8 #>  #> $error #> [1] 0.05 #>  #> $type #> [1] \"pfer\""},{"path":"https://fbertran.github.io/c060/index.html","id":"gaussian-stability-paths","dir":"","previous_headings":"","what":"Gaussian Stability Paths","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"plot chunk unnamed-chunk-5","code":"set.seed(1234) x <- matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000] %*% c(rep(2,5),rep(-2,5),rep(.1,990)) res <- stabpath(y,x,weakness=1,mc.cores=2) plot(res) #> Error in t.default(x$x): argument is not a matrix"},{"path":"https://fbertran.github.io/c060/index.html","id":"binomial-stability-paths","dir":"","previous_headings":"","what":"Binomial Stability Paths","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"plot chunk unnamed-chunk-6","code":"y=sample(1:2,100,replace=TRUE) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"binomial\") plot(res) #> Error in t.default(x$x): argument is not a matrix"},{"path":"https://fbertran.github.io/c060/index.html","id":"multinomial-stability-paths","dir":"","previous_headings":"","what":"Multinomial Stability Paths","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"plot chunk unnamed-chunk-7","code":"y=sample(1:4,100,replace=TRUE) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"multinomial\") #> Warning in mclapply(1:steps, mc.cores = mc.cores, glmnet.subset, subsets, : #> all scheduled cores encountered errors in user code #> Error in res[[1]]: subscript out of bounds plot(res) #> Error in t.default(x$x): argument is not a matrix"},{"path":"https://fbertran.github.io/c060/index.html","id":"poisson-stability-paths","dir":"","previous_headings":"","what":"Poisson Stability Paths","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"plot chunk unnamed-chunk-8","code":"N=100; p=1000 nzc=5 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) f = x[,seq(nzc)] %*% beta mu=exp(f) y=rpois(N,mu) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"poisson\") plot(res) #> Error in t.default(x$x): argument is not a matrix"},{"path":"https://fbertran.github.io/c060/index.html","id":"cox-stability-paths","dir":"","previous_headings":"","what":"Cox Stability Paths","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"plot chunk unnamed-chunk-9","code":"library(survival) set.seed(10101) N=100;p=1000 nzc=p/3 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) fx=x[,seq(nzc)] %*% beta/3 hx=exp(fx) ty=rexp(N,hx) tcens=rbinom(n=N,prob=.3,size=1) y=cbind(time=ty,status=1-tcens) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"cox\") plot(res) #> Error in t.default(x$x): argument is not a matrix"},{"path":"https://fbertran.github.io/c060/index.html","id":"example-from-glmnet-package","dir":"","previous_headings":"","what":"Example from glmnet package","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"","code":"set.seed(10101) library(glmnet) library(survival) library(peperr) N=1000;p=30 nzc=p/3 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) fx=x[,seq(nzc)] %*% beta/3 hx=exp(fx) ty=rexp(N,hx) tcens=rbinom(n=N,prob=.3,size=1)# censoring indicator y=Surv(ty,1-tcens)"},{"path":"https://fbertran.github.io/c060/index.html","id":"epsgo","dir":"","previous_headings":"","what":"EPSGO","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"","code":"set.seed(1010) n=1000;p=100 nzc=trunc(p/10) x=matrix(rnorm(n*p),n,p) beta=rnorm(nzc) fx= x[,seq(nzc)] %*% beta eps=rnorm(n)*5 y=drop(fx+eps) px=exp(fx) px=px/(1+px) ly=rbinom(n=length(px),prob=px,size=1) set.seed(1011)"},{"path":"https://fbertran.github.io/c060/index.html","id":"y---binomial","dir":"","previous_headings":"","what":"y - binomial","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"plot chunk unnamed-chunk-13","code":"y.classes<-ifelse(y>= median(y),1, 0) set.seed(1234) nfolds = 10 foldid <- balancedFolds(class.column.factor=y.classes, cross.outer=nfolds) #> 121  #> 2  #> 3  #> 4  #> 5  #> 6  #> 7  #> 8  #> 9  #> 10 bounds <- t(data.frame(alpha=c(0, 1))) colnames(bounds)<-c(\"lower\",\"upper\")   fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"final\",              fminlower = -100,              x = x, y = y.classes, family = \"binomial\",               foldid = foldid,              my.mfrow = c(4, 4),              type.min = \"lambda.1se\",              type.measure = \"mse\",              verbose = FALSE) #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.04184 #>     Xtrain    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>      alpha #> [1,]     0 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.01859 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.03437 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.02538 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.00565 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.02255 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.00881 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.01515 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.00258 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> 30 0.01515 0.4539218 #> [1] \"No changes in the last 10 iterations, break iterations\" summary(fit) #>             Length Class      Mode #> fmin         1     -none-     numeric #> xmin         1     -none-     numeric #> iter         1     -none-     numeric #> neval        1     -none-     numeric #> maxevals     1     -none-     numeric #> seed         1     -none-     numeric #> bounds       2     -none-     numeric #> Q.func       1     -none-     character #> points.fmin  2     data.frame list #> Xtrain      31     -none-     numeric #> Ytrain      31     -none-     numeric #> gp.seed     10     -none-     numeric #> model.list  31     -none-     list"},{"path":"https://fbertran.github.io/c060/index.html","id":"y---multinomial-low---low-25-middle---2575-quantiles-high---larger-75","dir":"","previous_headings":"","what":"y - multinomial: low - low 25%, middle - (25,75)-quantiles, high - larger 75%.","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"","code":"y.classes<-ifelse(y <= quantile(y,0.25),1, ifelse(y >= quantile(y,0.75),3, 2)) set.seed(1234) nfolds = 10 foldid <- balancedFolds(class.column.factor=y.classes, cross.outer=nfolds) #> 1231  #> 2  #> 3  #> 4  #> 5  #> 6  #> 7  #> 8  #> 9  #> 10 bounds <- t(data.frame(alpha=c(0, 1))) colnames(bounds)<-c(\"lower\",\"upper\")   fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"none\",              fminlower = -100,              x = x, y = y.classes, family = \"multinomial\",               foldid = foldid,              type.min = \"lambda.1se\",              type.measure = \"mse\",              verbose = FALSE) #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>       alpha #> [1,] 0.0605 #>     Xtrain    Ytrain #> 1  0.73445 0.5889776 #> 2  0.85311 0.5884918 #> 3  0.53235 0.5902795 #> 4  0.13097 0.5943578 #> 5  0.60814 0.5896935 #> 6  0.62236 0.5895989 #> 7  0.99216 0.5880701 #> 8  0.26607 0.5919904 #> 9  0.31969 0.5906950 #> 10 0.15851 0.5943488 #> 11 0.67349 0.5892911 #> 12 0.86540 0.5884489 #> 13 0.41169 0.5891854 #> 14 0.90698 0.5883127 #> 15 0.01183 0.6058205 #> 16 0.48959 0.5883259 #> 17 0.22937 0.5911021 #> 18 0.35578 0.5900189 #> 19 0.09411 0.5962309 #> 20 0.45474 0.5886749 #> 21 0.77450 0.5887978 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.03825 #>      alpha    Ytrain #> 1  0.73445 0.5889776 #> 2  0.85311 0.5884918 #> 3  0.53235 0.5902795 #> 4  0.13097 0.5943578 #> 5  0.60814 0.5896935 #> 6  0.62236 0.5895989 #> 7  0.99216 0.5880701 #> 8  0.26607 0.5919904 #> 9  0.31969 0.5906950 #> 10 0.15851 0.5943488 #> 11 0.67349 0.5892911 #> 12 0.86540 0.5884489 #> 13 0.41169 0.5891854 #> 14 0.90698 0.5883127 #> 15 0.01183 0.6058205 #> 16 0.48959 0.5883259 #> 17 0.22937 0.5911021 #> 18 0.35578 0.5900189 #> 19 0.09411 0.5962309 #> 20 0.45474 0.5886749 #> 21 0.77450 0.5887978 #> 22 0.06050 0.5985227 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.07476 #>      alpha    Ytrain #> 1  0.73445 0.5889776 #> 2  0.85311 0.5884918 #> 3  0.53235 0.5902795 #> 4  0.13097 0.5943578 #> 5  0.60814 0.5896935 #> 6  0.62236 0.5895989 #> 7  0.99216 0.5880701 #> 8  0.26607 0.5919904 #> 9  0.31969 0.5906950 #> 10 0.15851 0.5943488 #> 11 0.67349 0.5892911 #> 12 0.86540 0.5884489 #> 13 0.41169 0.5891854 #> 14 0.90698 0.5883127 #> 15 0.01183 0.6058205 #> 16 0.48959 0.5883259 #> 17 0.22937 0.5911021 #> 18 0.35578 0.5900189 #> 19 0.09411 0.5962309 #> 20 0.45474 0.5886749 #> 21 0.77450 0.5887978 #> 22 0.06050 0.5985227 #> 23 0.03825 0.5998758 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>      alpha #> [1,]     0 #>      alpha    Ytrain #> 1  0.73445 0.5889776 #> 2  0.85311 0.5884918 #> 3  0.53235 0.5902795 #> 4  0.13097 0.5943578 #> 5  0.60814 0.5896935 #> 6  0.62236 0.5895989 #> 7  0.99216 0.5880701 #> 8  0.26607 0.5919904 #> 9  0.31969 0.5906950 #> 10 0.15851 0.5943488 #> 11 0.67349 0.5892911 #> 12 0.86540 0.5884489 #> 13 0.41169 0.5891854 #> 14 0.90698 0.5883127 #> 15 0.01183 0.6058205 #> 16 0.48959 0.5883259 #> 17 0.22937 0.5911021 #> 18 0.35578 0.5900189 #> 19 0.09411 0.5962309 #> 20 0.45474 0.5886749 #> 21 0.77450 0.5887978 #> 22 0.06050 0.5985227 #> 23 0.03825 0.5998758 #> 24 0.07476 0.5974274 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.02692 #>      alpha    Ytrain #> 1  0.73445 0.5889776 #> 2  0.85311 0.5884918 #> 3  0.53235 0.5902795 #> 4  0.13097 0.5943578 #> 5  0.60814 0.5896935 #> 6  0.62236 0.5895989 #> 7  0.99216 0.5880701 #> 8  0.26607 0.5919904 #> 9  0.31969 0.5906950 #> 10 0.15851 0.5943488 #> 11 0.67349 0.5892911 #> 12 0.86540 0.5884489 #> 13 0.41169 0.5891854 #> 14 0.90698 0.5883127 #> 15 0.01183 0.6058205 #> 16 0.48959 0.5883259 #> 17 0.22937 0.5911021 #> 18 0.35578 0.5900189 #> 19 0.09411 0.5962309 #> 20 0.45474 0.5886749 #> 21 0.77450 0.5887978 #> 22 0.06050 0.5985227 #> 23 0.03825 0.5998758 #> 24 0.07476 0.5974274 #> 25 0.00000 0.6118007 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"the differences in functions between 2 last iterations is small, stop iterations\" #> [1] \"finished? TRUE\" #> [1] \"X\" #>        alpha #> [1,] 0.02692 #>      alpha    Ytrain #> 1  0.73445 0.5889776 #> 2  0.85311 0.5884918 #> 3  0.53235 0.5902795 #> 4  0.13097 0.5943578 #> 5  0.60814 0.5896935 #> 6  0.62236 0.5895989 #> 7  0.99216 0.5880701 #> 8  0.26607 0.5919904 #> 9  0.31969 0.5906950 #> 10 0.15851 0.5943488 #> 11 0.67349 0.5892911 #> 12 0.86540 0.5884489 #> 13 0.41169 0.5891854 #> 14 0.90698 0.5883127 #> 15 0.01183 0.6058205 #> 16 0.48959 0.5883259 #> 17 0.22937 0.5911021 #> 18 0.35578 0.5900189 #> 19 0.09411 0.5962309 #> 20 0.45474 0.5886749 #> 21 0.77450 0.5887978 #> 22 0.06050 0.5985227 #> 23 0.03825 0.5998758 #> 24 0.07476 0.5974274 #> 25 0.00000 0.6118007 #> 26 0.02692 0.6012652 summary(fit) #>             Length Class      Mode      #> fmin         1     -none-     numeric   #> xmin         1     -none-     numeric   #> iter         1     -none-     numeric   #> neval        1     -none-     numeric   #> maxevals     1     -none-     numeric   #> seed         1     -none-     numeric   #> bounds       2     -none-     numeric   #> Q.func       1     -none-     character #> points.fmin  2     data.frame list      #> Xtrain      26     -none-     numeric   #> Ytrain      26     -none-     numeric   #> gp.seed      6     -none-     numeric   #> model.list  26     -none-     list"},{"path":"https://fbertran.github.io/c060/index.html","id":"gaussian","dir":"","previous_headings":"","what":"Gaussian","title":"Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models","text":"","code":"set.seed(1234) x=matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000]%*%c(rep(2,5),rep(-2,5),rep(.1,990))  foldid <- rep(1:10,each=10)  fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"none\",              fminlower = -100,              x = x, y = y, family = \"gaussian\",               foldid = foldid,              type.min = \"lambda.1se\",              type.measure = \"mse\",              verbose = FALSE) #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>      alpha #> [1,]     1 #>     Xtrain   Ytrain #> 1  0.73445 25.59440 #> 2  0.85311 25.34047 #> 3  0.53235 26.20553 #> 4  0.13097 32.02254 #> 5  0.60814 25.93154 #> 6  0.62236 25.88775 #> 7  0.99216 25.10230 #> 8  0.26607 29.01779 #> 9  0.31969 28.01304 #> 10 0.15851 31.00683 #> 11 0.67349 25.74568 #> 12 0.86540 25.31520 #> 13 0.41169 26.83464 #> 14 0.90698 25.23994 #> 15 0.01183 38.56498 #> 16 0.48959 26.40450 #> 17 0.22937 29.94172 #> 18 0.35578 27.74582 #> 19 0.09411 32.97585 #> 20 0.45474 26.58546 #> 21 0.77450 25.49951 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.97776 #>      alpha   Ytrain #> 1  0.73445 25.59440 #> 2  0.85311 25.34047 #> 3  0.53235 26.20553 #> 4  0.13097 32.02254 #> 5  0.60814 25.93154 #> 6  0.62236 25.88775 #> 7  0.99216 25.10230 #> 8  0.26607 29.01779 #> 9  0.31969 28.01304 #> 10 0.15851 31.00683 #> 11 0.67349 25.74568 #> 12 0.86540 25.31520 #> 13 0.41169 26.83464 #> 14 0.90698 25.23994 #> 15 0.01183 38.56498 #> 16 0.48959 26.40450 #> 17 0.22937 29.94172 #> 18 0.35578 27.74582 #> 19 0.09411 32.97585 #> 20 0.45474 26.58546 #> 21 0.77450 25.49951 #> 22 1.00000 25.09094 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.95773 #>      alpha   Ytrain #> 1  0.73445 25.59440 #> 2  0.85311 25.34047 #> 3  0.53235 26.20553 #> 4  0.13097 32.02254 #> 5  0.60814 25.93154 #> 6  0.62236 25.88775 #> 7  0.99216 25.10230 #> 8  0.26607 29.01779 #> 9  0.31969 28.01304 #> 10 0.15851 31.00683 #> 11 0.67349 25.74568 #> 12 0.86540 25.31520 #> 13 0.41169 26.83464 #> 14 0.90698 25.23994 #> 15 0.01183 38.56498 #> 16 0.48959 26.40450 #> 17 0.22937 29.94172 #> 18 0.35578 27.74582 #> 19 0.09411 32.97585 #> 20 0.45474 26.58546 #> 21 0.77450 25.49951 #> 22 1.00000 25.09094 #> 23 0.97776 25.12374 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.98756 #>      alpha   Ytrain #> 1  0.73445 25.59440 #> 2  0.85311 25.34047 #> 3  0.53235 26.20553 #> 4  0.13097 32.02254 #> 5  0.60814 25.93154 #> 6  0.62236 25.88775 #> 7  0.99216 25.10230 #> 8  0.26607 29.01779 #> 9  0.31969 28.01304 #> 10 0.15851 31.00683 #> 11 0.67349 25.74568 #> 12 0.86540 25.31520 #> 13 0.41169 26.83464 #> 14 0.90698 25.23994 #> 15 0.01183 38.56498 #> 16 0.48959 26.40450 #> 17 0.22937 29.94172 #> 18 0.35578 27.74582 #> 19 0.09411 32.97585 #> 20 0.45474 26.58546 #> 21 0.77450 25.49951 #> 22 1.00000 25.09094 #> 23 0.97776 25.12374 #> 24 0.95773 25.15577 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.80954 #>      alpha   Ytrain #> 1  0.73445 25.59440 #> 2  0.85311 25.34047 #> 3  0.53235 26.20553 #> 4  0.13097 32.02254 #> 5  0.60814 25.93154 #> 6  0.62236 25.88775 #> 7  0.99216 25.10230 #> 8  0.26607 29.01779 #> 9  0.31969 28.01304 #> 10 0.15851 31.00683 #> 11 0.67349 25.74568 #> 12 0.86540 25.31520 #> 13 0.41169 26.83464 #> 14 0.90698 25.23994 #> 15 0.01183 38.56498 #> 16 0.48959 26.40450 #> 17 0.22937 29.94172 #> 18 0.35578 27.74582 #> 19 0.09411 32.97585 #> 20 0.45474 26.58546 #> 21 0.77450 25.49951 #> 22 1.00000 25.09094 #> 23 0.97776 25.12374 #> 24 0.95773 25.15577 #> 25 0.98756 25.10930 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"the differences in functions between 2 last iterations is small, stop iterations\" #> [1] \"finished? TRUE\" #> [1] \"X\" #>        alpha #> [1,] 0.80954 #>      alpha   Ytrain #> 1  0.73445 25.59440 #> 2  0.85311 25.34047 #> 3  0.53235 26.20553 #> 4  0.13097 32.02254 #> 5  0.60814 25.93154 #> 6  0.62236 25.88775 #> 7  0.99216 25.10230 #> 8  0.26607 29.01779 #> 9  0.31969 28.01304 #> 10 0.15851 31.00683 #> 11 0.67349 25.74568 #> 12 0.86540 25.31520 #> 13 0.41169 26.83464 #> 14 0.90698 25.23994 #> 15 0.01183 38.56498 #> 16 0.48959 26.40450 #> 17 0.22937 29.94172 #> 18 0.35578 27.74582 #> 19 0.09411 32.97585 #> 20 0.45474 26.58546 #> 21 0.77450 25.49951 #> 22 1.00000 25.09094 #> 23 0.97776 25.12374 #> 24 0.95773 25.15577 #> 25 0.98756 25.10930 #> 26 0.80954 25.42666 summary(fit)  #>             Length Class      Mode      #> fmin         1     -none-     numeric   #> xmin         1     -none-     numeric   #> iter         1     -none-     numeric   #> neval        1     -none-     numeric   #> maxevals     1     -none-     numeric   #> seed         1     -none-     numeric   #> bounds       2     -none-     numeric   #> Q.func       1     -none-     character #> points.fmin  2     data.frame list      #> Xtrain      26     -none-     numeric   #> Ytrain      26     -none-     numeric   #> gp.seed      6     -none-     numeric   #> model.list  26     -none-     list"},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":null,"dir":"Reference","previous_headings":"","what":"Efficient Parameter Selection via Global Optimization — EPSGO","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"Finds optimal solution Q.func function.","code":""},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"","code":"EPSGO(   Q.func,   bounds,   round.n = 5,   parms.coding = \"none\",   fminlower = 0,   flag.find.one.min = FALSE,   show = c(\"none\", \"final\", \"all\"),   N = NULL,   maxevals = 500,   pdf.name = NULL,   pdf.width = 12,   pdf.height = 12,   my.mfrow = c(1, 1),   verbose = TRUE,   seed = 123,   ... )"},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"Q.func name function minimized. bounds bounds parameters round.n number digits comma, default: 5 parms.coding parmeters coding: none log2, default: none. fminlower minimal value function Q.func, default 0. flag.find.one.min want find one min value stop? Default: FALSE show show plots DIRECT algorithm: none, final iteration, iterations. Default: none N define number start points, see details. maxevals maximum number DIRECT function evaluations, default: 500. pdf.name pdf name pdf.width default: 12 pdf.height default: 12 .mfrow default: c(1,1) verbose verbose? default: TRUE. seed seed ... additional argument(s)","code":""},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"fmin minimal value Q.func interval defined bounds. xmin corresponding parameters minimum iter number iterations neval number visited points maxevals maximum number DIRECT function evaluations seed seed bounds bounds parameters Q.func name function minimized. points.fmin set points fmin Xtrain visited points Ytrain output Q.func visited points Xtrain gp.seed seed Gaussian Process model.list detailed information search process","code":""},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"number start points (N) defined user, defined dependent dimensionality parameter space.  N=10D+1, D number parameters, high dimensional parameter space 6 dimensions, initial set restricted 65. However one-dimensional parameter space N set 21 due stability reasons. idea EPSGO (Efficient Parameter Selection via Global Optimization): Beginning intial Latin hypercube sampling containing N starting points train Online GP, look point maximal expected improvement, sample update Gaussian Process(GP). Thereby important GP really correctly models error surface SVM parameter space, can give us information potentially interesting points parameter space sample next.  continue sampling points convergence criterion met. DIRECT sampling algorithm requires knowledge objective function gradient.  Instead, algorithm samples points domain, uses information obtained decide search next. DIRECT algorithm globally converge maximal value objective function. name DIRECT comes shortening phrase 'DIviding RECTangles', describes way algorithm moves towards optimum. code source adopted MATLAB originals, special thanks Holger Froehlich.","code":""},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"Froehlich, H. Zell, . (2005) \"Effcient parameter selection support vector machines classification regression via model-based global optimization\" Proc. Int. Joint Conf. Neural Networks, 1431-1438 . Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"Natalia Becker natalia.becker dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/EPSGO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Efficient Parameter Selection via Global Optimization — EPSGO","text":"","code":"set.seed(1010) n=1000;p=100 nzc=trunc(p/10) x=matrix(rnorm(n*p),n,p) beta=rnorm(nzc) fx= x[,seq(nzc)] %*% beta eps=rnorm(n)*5 y=drop(fx+eps) px=exp(fx) px=px/(1+px) ly=rbinom(n=length(px),prob=px,size=1) set.seed(1011)  # \\donttest{ # y - binomial y.classes<-ifelse(y>= median(y),1, 0) set.seed(1234) nfolds = 10 foldid <- balancedFolds(class.column.factor=y.classes, cross.outer=nfolds) #> 121  #> 2  #> 3  #> 4  #> 5  #> 6  #> 7  #> 8  #> 9  #> 10  bounds <- t(data.frame(alpha=c(0, 1))) colnames(bounds)<-c(\"lower\",\"upper\")   fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"none\",              fminlower = -100,              x = x, y = y.classes, family = \"binomial\",               foldid = foldid,              type.min = \"lambda.1se\",              type.measure = \"mse\") #> [1] \"parms.coding\" #> [1] \"none\" #>          [,1] #>  [1,] 0.73445 #>  [2,] 0.85311 #>  [3,] 0.53235 #>  [4,] 0.13097 #>  [5,] 0.60814 #>  [6,] 0.62236 #>  [7,] 0.99216 #>  [8,] 0.26607 #>  [9,] 0.31969 #> [10,] 0.15851 #> [11,] 0.67349 #> [12,] 0.86540 #> [13,] 0.41169 #> [14,] 0.90698 #> [15,] 0.01183 #> [16,] 0.48959 #> [17,] 0.22937 #> [18,] 0.35578 #> [19,] 0.09411 #> [20,] 0.45474 #> [21,] 0.77450 #> [1] \"alpha= 0.73445\" #> [1] \"alpha= 0.85311\" #> [1] \"alpha= 0.53235\" #> [1] \"alpha= 0.13097\" #> [1] \"alpha= 0.60814\" #> [1] \"alpha= 0.62236\" #> [1] \"alpha= 0.99216\" #> [1] \"alpha= 0.26607\" #> [1] \"alpha= 0.31969\" #> [1] \"alpha= 0.15851\" #> [1] \"alpha= 0.67349\" #> [1] \"alpha= 0.8654\" #> [1] \"alpha= 0.41169\" #> [1] \"alpha= 0.90698\" #> [1] \"alpha= 0.01183\" #> [1] \"alpha= 0.48959\" #> [1] \"alpha= 0.22937\" #> [1] \"alpha= 0.35578\" #> [1] \"alpha= 0.09411\" #> [1] \"alpha= 0.45474\" #> [1] \"alpha= 0.7745\" #>          X         Q #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"loop 1\" #> [1] \"fmin= 0.441921792409136\" #> [1] \"EImax= 0.360494188095381\" #> [1] \"xmax:\" #>            upper #> alpha 0.04183813 #> [1] \"history:\" #>       Iteration Nr Function Count      f_min #>  [1,]            1              3  0.0000000 #>  [2,]            2              9  0.0000000 #>  [3,]            3             27 -0.2824064 #>  [4,]            4             29 -0.3465652 #>  [5,]            5             83 -0.3533452 #>  [6,]            6             87 -0.3551043 #>  [7,]            7             93 -0.3556474 #>  [8,]            8            255 -0.3580834 #>  [9,]            9            257 -0.3593507 #> [10,]           10            261 -0.3597449 #> [11,]           11            267 -0.3598733 #> [12,]           12            275 -0.3599158 #> [13,]           13            285 -0.3599299 #> [14,]           14            297 -0.3599347 #> [15,]           15            311 -0.3599362 #> [16,]           16            791 -0.3604942 #> [1] \"iteration : 1   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.04184 #>     Xtrain    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> [1] \"loop 2\" #> [1] \"alpha= 0.04184\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.28764932113945\" #> [1] \"xmax:\" #>              upper #> alpha 9.408382e-07 #> [1] \"history:\" #>       Iteration Nr Function Count      f_min #>  [1,]            1              3  0.0000000 #>  [2,]            2              9  0.0000000 #>  [3,]            3             27 -0.1696142 #>  [4,]            4             29 -0.1891384 #>  [5,]            5             83 -0.2656889 #>  [6,]            6             87 -0.2810880 #>  [7,]            7            249 -0.2855411 #>  [8,]            8            255 -0.2869601 #>  [9,]            9            261 -0.2874261 #> [10,]           10            271 -0.2875807 #> [11,]           11            283 -0.2876322 #> [12,]           12            769 -0.2876493 #> [1] \"iteration : 2   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>      alpha #> [1,]     0 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> [1] \"loop 3\" #> [1] \"alpha= 0\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.288438449825165\" #> [1] \"xmax:\" #>            upper #> alpha 0.01859379 #> [1] \"history:\" #>       Iteration Nr Function Count      f_min #>  [1,]            1              3  0.0000000 #>  [2,]            2              9  0.0000000 #>  [3,]            3             27 -0.2870820 #>  [4,]            4             29 -0.2870820 #>  [5,]            5             83 -0.2870820 #>  [6,]            6             87 -0.2870820 #>  [7,]            7            247 -0.2870820 #>  [8,]            8            253 -0.2870820 #>  [9,]            9            259 -0.2880008 #> [10,]           10            267 -0.2883041 #> [11,]           11            277 -0.2884049 #> [12,]           12            763 -0.2884384 #> [1] \"iteration : 3   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.01859 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> [1] \"loop 4\" #> [1] \"alpha= 0.01859\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.376644447641587\" #> [1] \"xmax:\" #>            upper #> alpha 0.03436934 #> [1] \"history:\" #>       Iteration Nr Function Count         f_min #>  [1,]            1              3  0.000000e+00 #>  [2,]            2              9  0.000000e+00 #>  [3,]            3             27 -8.834395e-08 #>  [4,]            4             29 -3.743045e-01 #>  [5,]            5             83 -3.765597e-01 #>  [6,]            6             87 -3.765597e-01 #>  [7,]            7             93 -3.766393e-01 #>  [8,]            8            255 -3.766444e-01 #>  [9,]            9            263 -3.766444e-01 #> [10,]           10            273 -3.766444e-01 #> [11,]           11            285 -3.766444e-01 #> [12,]           12            299 -3.766444e-01 #> [13,]           13            315 -3.766444e-01 #> [14,]           14            333 -3.766444e-01 #> [15,]           15            817 -3.766444e-01 #> [1] \"iteration : 4   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.03437 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> [1] \"loop 5\" #> [1] \"alpha= 0.03437\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.365043353841314\" #> [1] \"xmax:\" #>            upper #> alpha 0.02537723 #> [1] \"history:\" #>       Iteration Nr Function Count         f_min #>  [1,]            1              3  0.000000e+00 #>  [2,]            2              9  0.000000e+00 #>  [3,]            3             27 -1.551645e-05 #>  [4,]            4             29 -3.332488e-01 #>  [5,]            5             83 -3.332488e-01 #>  [6,]            6             87 -3.449330e-01 #>  [7,]            7            249 -3.579414e-01 #>  [8,]            8            253 -3.607789e-01 #>  [9,]            9            259 -3.616089e-01 #> [10,]           10            265 -3.618739e-01 #> [11,]           11            275 -3.619609e-01 #> [12,]           12            759 -3.650434e-01 #> [1] \"iteration : 5   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.02538 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> [1] \"loop 6\" #> [1] \"alpha= 0.02538\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.33165134229888\" #> [1] \"xmax:\" #>             upper #> alpha 0.005647852 #> [1] \"history:\" #>       Iteration Nr Function Count         f_min #>  [1,]            1              3  0.000000e+00 #>  [2,]            2              9  0.000000e+00 #>  [3,]            3             27 -2.058179e-11 #>  [4,]            4             29 -3.310340e-01 #>  [5,]            5             83 -3.310340e-01 #>  [6,]            6             87 -3.310340e-01 #>  [7,]            7            249 -3.316411e-01 #>  [8,]            8            255 -3.316411e-01 #>  [9,]            9            263 -3.316507e-01 #> [10,]           10            273 -3.316513e-01 #> [11,]           11            285 -3.316513e-01 #> [12,]           12            771 -3.316513e-01 #> [1] \"iteration : 6   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.00565 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> [1] \"loop 7\" #> [1] \"alpha= 0.00565\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.169139300761214\" #> [1] \"xmax:\" #>            upper #> alpha 0.02254907 #> [1] \"history:\" #>       Iteration Nr Function Count         f_min #>  [1,]            1              3  0.000000e+00 #>  [2,]            2              9  0.000000e+00 #>  [3,]            3             27 -3.120980e-16 #>  [4,]            4             81 -1.283425e-03 #>  [5,]            5             83 -9.553696e-02 #>  [6,]            6            245 -1.690474e-01 #>  [7,]            7            247 -1.690474e-01 #>  [8,]            8            251 -1.690474e-01 #>  [9,]            9            257 -1.690576e-01 #> [10,]           10            265 -1.691317e-01 #> [11,]           11            747 -1.691393e-01 #> [1] \"iteration : 7   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.02255 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> [1] \"loop 8\" #> [1] \"alpha= 0.02255\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.298921709400262\" #> [1] \"xmax:\" #>             upper #> alpha 0.008814713 #> [1] \"history:\" #>       Iteration Nr Function Count         f_min #>  [1,]            1              3  0.000000e+00 #>  [2,]            2              9  0.000000e+00 #>  [3,]            3             27 -2.719416e-08 #>  [4,]            4             29 -1.207255e-01 #>  [5,]            5             83 -2.721111e-01 #>  [6,]            6             87 -2.721111e-01 #>  [7,]            7            249 -2.764528e-01 #>  [8,]            8            253 -2.987879e-01 #>  [9,]            9            257 -2.987879e-01 #> [10,]           10            263 -2.988964e-01 #> [11,]           11            745 -2.989217e-01 #> [1] \"iteration : 8   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.00881 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> [1] \"loop 9\" #> [1] \"alpha= 0.00881\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.358893503444292\" #> [1] \"xmax:\" #>            upper #> alpha 0.01514844 #> [1] \"history:\" #>       Iteration Nr Function Count        f_min #>  [1,]            1              3  0.000000000 #>  [2,]            2              9  0.000000000 #>  [3,]            3             27 -0.007906843 #>  [4,]            4             29 -0.131613696 #>  [5,]            5             83 -0.334659883 #>  [6,]            6             87 -0.355827428 #>  [7,]            7            249 -0.356846019 #>  [8,]            8            253 -0.358737403 #>  [9,]            9            259 -0.358890975 #> [10,]           10            267 -0.358890975 #> [11,]           11            751 -0.358893503 #> [1] \"iteration : 9   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.01515 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> [1] \"loop 10\" #> [1] \"alpha= 0.01515\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> 30 0.01515 0.4539218 #> [1] \"fmin= 0.441921792409136\" #> ...done #> ...done #> ...done #> ...done #> ...done #> [1] \"EImax= 0.344941817080621\" #> [1] \"xmax:\" #>             upper #> alpha 0.002584483 #> [1] \"history:\" #>       Iteration Nr Function Count         f_min #>  [1,]            1              3  0.0000000000 #>  [2,]            2              9  0.0000000000 #>  [3,]            3             27 -0.0001563221 #>  [4,]            4             29 -0.1340091716 #>  [5,]            5             83 -0.3425694657 #>  [6,]            6             87 -0.3425694657 #>  [7,]            7            247 -0.3449022032 #>  [8,]            8            253 -0.3449022032 #>  [9,]            9            259 -0.3449390412 #> [10,]           10            267 -0.3449418038 #> [11,]           11            279 -0.3449418038 #> [12,]           12            767 -0.3449418171 #> [1] \"iteration : 10   fmin =  0.441921792409136\" #> [1] \"finished? FALSE\" #> [1] \"X\" #>        alpha #> [1,] 0.00258 #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> 30 0.01515 0.4539218 #> [1] \"loop 11\" #> [1] \"alpha= 0.00258\" #>      alpha    Ytrain #> 1  0.73445 0.4427822 #> 2  0.85311 0.4422716 #> 3  0.53235 0.4419218 #> 4  0.13097 0.4444470 #> 5  0.60814 0.4435479 #> 6  0.62236 0.4434463 #> 7  0.99216 0.4438921 #> 8  0.26607 0.4440248 #> 9  0.31969 0.4426631 #> 10 0.15851 0.4444240 #> 11 0.67349 0.4431161 #> 12 0.86540 0.4422268 #> 13 0.41169 0.4432506 #> 14 0.90698 0.4420846 #> 15 0.01183 0.4534574 #> 16 0.48959 0.4423172 #> 17 0.22937 0.4430704 #> 18 0.35578 0.4419665 #> 19 0.09411 0.4462451 #> 20 0.45474 0.4426950 #> 21 0.77450 0.4425921 #> 22 0.04184 0.4502101 #> 23 0.00000 0.4563529 #> 24 0.01859 0.4535900 #> 25 0.03437 0.4498726 #> 26 0.02538 0.4515460 #> 27 0.00565 0.4550679 #> 28 0.02255 0.4518852 #> 29 0.00881 0.4553102 #> 30 0.01515 0.4539218 #> 31 0.00258 0.4558921 #> [1] \"fmin= 0.441921792409136\" #> [1] \"No changes in the last 10 iterations, break iterations\" summary(fit) #>             Length Class      Mode      #> fmin         1     -none-     numeric   #> xmin         1     -none-     numeric   #> iter         1     -none-     numeric   #> neval        1     -none-     numeric   #> maxevals     1     -none-     numeric   #> seed         1     -none-     numeric   #> bounds       2     -none-     numeric   #> Q.func       1     -none-     character #> points.fmin  2     data.frame list      #> Xtrain      31     -none-     numeric   #> Ytrain      31     -none-     numeric   #> gp.seed     10     -none-     numeric   #> model.list  31     -none-     list      # }  if (FALSE) { # \\dontrun{ # y - multinomial: low - low 25%, middle - (25,75)-quantiles, high - larger 75%. y.classes<-ifelse(y <= quantile(y,0.25),1, ifelse(y >= quantile(y,0.75),3, 2)) set.seed(1234) nfolds = 10 foldid <- balancedFolds(class.column.factor=y.classes, cross.outer=nfolds) bounds <- t(data.frame(alpha=c(0, 1))) colnames(bounds)<-c(\"lower\",\"upper\")   fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"none\",              fminlower = -100,              x = x, y = y.classes, family = \"multinomial\",               foldid = foldid,              type.min = \"lambda.1se\",              type.measure = \"mse\") summary(fit) } # }  if (FALSE) { # \\dontrun{ ##poisson N=500; p=20 nzc=5 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) f = x[,seq(nzc)]%*%beta mu=exp(f) y.classes=rpois(N,mu)  nfolds = 10 set.seed(1234) foldid <- balancedFolds(class.column.factor=y.classes, cross.outer=nfolds)  fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"none\",              fminlower = -100,              x = x, y = y.classes, family = \"poisson\",               foldid = foldid,              type.min = \"lambda.1se\",              type.measure = \"mse\") summary(fit) } # }  if (FALSE) { # \\dontrun{ #gaussian set.seed(1234) x=matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000]%*%c(rep(2,5),rep(-2,5),rep(.1,990))  foldid <- rep(1:10,each=10)  fit <- EPSGO(Q.func=\"tune.glmnet.interval\",               bounds=bounds,               parms.coding=\"none\",               seed = 1234,               show=\"none\",              fminlower = -100,              x = x, y = y, family = \"gaussian\",               foldid = foldid,              type.min = \"lambda.1se\",              type.measure = \"mse\") summary(fit)   } # }  # y - cox in vignette"},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"Extracts predictive partial log-likelihood glmnet Cox PH model fit.","code":""},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"","code":"# S3 method for class 'coxnet' PLL(object, newdata, newtime, newstatus, complexity, ...)"},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"object fitted model class coxnet. newdata n_new*p matrix covariates. newtime n_new-vector censored survival times. newstatus n_new-vector survival status, coded 0 .1 complexity lambda penalty value. ... additional arguments, used.","code":""},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"Vector length n_new","code":""},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"Used function peperr, function fit.glmnet family=\"cox\" used model fit, gives class coxnet object. basically wrapper based coxnet.deviance function package glmnet.","code":""},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":"https://fbertran.github.io/c060/reference/PLL.coxnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predictive partial log-likelihood for glmnet Cox PH model fit — PLL.coxnet","text":"Thomas Hielscher t.hielscher@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"Creates several plots showing coefficient path final model cv.glmnet fit highlights path pre-specified set variables within coefficient path.","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"","code":"Plot.coef.glmnet(cvfit, betas)"},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"cvfit object class \"cv.glmnet\" returned function cv.glmnet. betas vector names variables; must subset rownames(coef(cvfit)).","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"list four objects stable vector giving positions estimated stable variables lambda penalization parameter used stability selection lpos position penalization parameter regularization path error desired type error level w.r.t. chosen type error rate type type error rate","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"Manuela Zucknick \\ m.zucknick@dkfz-heidelberg.de","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.coef.glmnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"function to highlight the path of a pre-specified set of variables within the coefficient path — Plot.coef.glmnet","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(1010) n=1000;p=100 nzc=trunc(p/10) x=matrix(rnorm(n*p),n,p) beta=rnorm(nzc) fx= x[,seq(nzc)] %*% beta eps=rnorm(n)*5 y=drop(fx+eps) px=exp(fx) px=px/(1+px) ly=rbinom(n=length(px),prob=px,size=1) set.seed(1011) cvob1=cv.glmnet(x,y) Plot.coef.glmnet(cvob1, c(\"V1\",\"V100\")) } # }"},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"Plots individual aggregated prediction error estimates based bootstrap samples.","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"","code":"Plot.peperr.curves(   x,   at.risk = TRUE,   allErrors = FALSE,   bootRuns = FALSE,   bootQuants = TRUE,   bootQuants.level = 0.95,   leg.cex = 0.7,   ... )"},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"x peperr object. .risk number risk display. default TRUE. allErrors Display .632, information average --bag error addition. default FALSE. bootRuns Display individual --bag bootstrap samples. default FALSE. bootQuants Display pointwise --bag bootstrap quantiles shaded area. default TRUE. bootQuants.level Quantile probabilities pointwise --bag bootstrap quantiles. default 0.95, .e. 2.5% 97.5% quantiles. leg.cex size legend text ... additional arguments, used.","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"function literally taken plot.peperr peperr package. display prediction error curves adapted allow numbers risk pointwise bootstrap quantiles.","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"Thomas Hielscher t.hielscher@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/Plot.peperr.curves.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot method for prediction error curves of a peperr object — Plot.peperr.curves","text":"","code":"if (FALSE) { # \\dontrun{  # example from glmnet package set.seed(10101) library(glmnet) library(survival) library(peperr)  N=1000;p=30 nzc=p/3 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) fx=x[,seq(nzc)]%*%beta/3 hx=exp(fx) ty=rexp(N,hx) tcens=rbinom(n=N,prob=.3,size=1)# censoring indicator y=Surv(ty,1-tcens)  peperr.object <- peperr(response=y, x=x,                          fit.fun=fit.glmnet, args.fit=list(family=\"cox\"),                          complexity=complexity.glmnet,                           args.complexity=list(family=\"cox\",nfolds=10),                         indices=resample.indices(n=N, method=\"sub632\", sample.n=10))  # pointwise bootstrap quantiles and all error types Plot.peperr.curves(peperr.object, allErrors=TRUE)  # individual bootstrap runs and selected error types Plot.peperr.curves(peperr.object, allErrors=FALSE, bootRuns=TRUE) } # }"},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the area under the ROC curve for a fitted model — aggregation.auc","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"Evaluate area ROC curve fitted model new data. used argument aggregation.fun peperr call.","code":""},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"","code":"aggregation.auc(   full.data = NULL,   response,   x,   model,   cplx = NULL,   type = c(\"apparent\", \"noinf\"),   fullsample.attr = NULL,   ... )"},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"full.data passed peperr, used calculation. response vector binary response. x n*p matrix covariates. model model fitted returned fit.fun, used call peperr. cplx passed peperr, necessary calculation. type character. fullsample.attr passed peperr, necessary calculation. ... additional arguments, passed predict function.","code":""},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"Scalar, indicating area ROC curve.","code":""},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"Area ROC curve calculated based internal glmnet:::auc function package glmnet.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"Thomas Hielscher t.hielscher@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/aggregation.auc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the area under the ROC curve for a fitted model — aggregation.auc","text":"","code":"if (FALSE) { # \\dontrun{ # binomial model - classification  library(c060) library(gridExtra) library(ggplot2)  set.seed(0815) x <- matrix(rnorm(100*20),100,20) y <- sample(0:1,100,replace=TRUE)  peperr_obj <- peperr(response=y, x=x, fit.fun=fit.glmnet, args.fit=list(family=\"binomial\"),            complexity=complexity.glmnet, args.complexity=list(nfolds=10, family=\"binomial\"),            trace=F, RNG=\"fixed\",seed=0815, #           aggregation.fun=c060:::aggregation.misclass,                   #           aggregation.fun=c060:::aggregation.brier,                              aggregation.fun=c060:::aggregation.auc,                              indices=resample.indices(n=nrow(x), sample.n = 100, method = \"sub632\"))  tmp   <- data.frame(grp=\"\",error=unlist(peperr_obj$sample.error))  errs  <- data.frame(error=c(perr(peperr_obj,\"resample\"),          perr(peperr_obj,\"632p\"),perr(peperr_obj,\"app\"),          perr(peperr_obj,\"nullmodel\")), col  = c(\"red\",\"blue\",\"green\",\"brown\"),          row.names=c(\"mean\\nout-of-bag\",\".632plus\",\"apparent\",\"null model\"))                   p     <- ggplot(tmp, aes(grp,error)) pg    <- p + geom_boxplot(outlier.colour = rgb(0,0,0,0), outlier.size=0) +          geom_jitter(position=position_jitter(width=.1)) +           theme_bw() + scale_y_continuous(\"AUC\") +  scale_x_discrete(\"\") +          geom_hline(aes(yintercept=error, colour=col), data=errs, show_guide=T) +           scale_colour_identity(\"error type\", guide = \"legend\", breaks=errs$col,          labels=rownames(errs)) +          ggtitle(\"AUC \\n in bootstrap samples \")                         p2     <- ggplot(data.frame(complx=peperr_obj$sample.complexity), aes(x=complx)) pg2    <- p2 + geom_histogram(binwidth = 0.02, fill = \"white\", colour=\"black\") +           theme_bw()+  xlab(expression(lambda)) +           ylab(\"frequency\") +            geom_vline(xintercept=peperr_obj$selected.complexity, colour=\"red\") +            ggtitle(\"Selected complexity \\n in bootstrap samples\") +           ggplot2::annotate(\"text\", x = 0.12, y = -0.5,           label = \"full data\", colour=\"red\", size=4)  grid.arrange(pg2, pg, ncol=2)  } # }"},{"path":"https://fbertran.github.io/c060/reference/balancedFolds.html","id":null,"dir":"Reference","previous_headings":"","what":"Function producing stratified/ balanced folds for cross validation — balancedFolds","title":"Function producing stratified/ balanced folds for cross validation — balancedFolds","text":"Get balanced folds cross validation, used tuning penalization parameters","code":""},{"path":"https://fbertran.github.io/c060/reference/balancedFolds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function producing stratified/ balanced folds for cross validation — balancedFolds","text":"","code":"balancedFolds(class.column.factor, cross.outer)"},{"path":"https://fbertran.github.io/c060/reference/balancedFolds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function producing stratified/ balanced folds for cross validation — balancedFolds","text":"class.column.factor class labels length n cross.outer number folds","code":""},{"path":"https://fbertran.github.io/c060/reference/balancedFolds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function producing stratified/ balanced folds for cross validation — balancedFolds","text":"permutated.cut vector length n, indicating fold belongs model model list alpha - optimal alpha lambda - optimal lambda nfolds - cross-validation's folds cvreg - cv.glmnet object optimal alpha fit - glmnet object optimal alpha optimal lambda","code":""},{"path":"https://fbertran.github.io/c060/reference/balancedFolds.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Function producing stratified/ balanced folds for cross validation — balancedFolds","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/balancedFolds.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Function producing stratified/ balanced folds for cross validation — balancedFolds","text":"Natalia Becker natalia.becker dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/c060-package.html","id":null,"dir":"Reference","previous_headings":"","what":"c060: Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models — c060-package","title":"c060: Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models — c060-package","text":"c060 package provides additional functions perform stability selection, model validation parameter tuning glmnet models.","code":""},{"path":"https://fbertran.github.io/c060/reference/c060-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"c060: Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models — c060-package","text":"Friedman, J., Hastie, T. Tibshirani, R. (2008) Regularization Paths Generalized Linear Models via Coordinate Descent, https://web.stanford.edu/~hastie/Papers/glmnet.pdfJournal Statistical Software, Vol. 33(1), 1-22 Feb 2010https://www.jstatsoft.org/v33/i01/ Simon, N., Friedman, J., Hastie, T., Tibshirani, R. (2011) Regularization Paths Cox's Proportional Hazards Model via Coordinate Descent, Journal Statistical Software, Vol. 39(5) 1-13https://www.jstatsoft.org/v39/i05/ Porzelius, C., Binder, H., Schumacher, M. (2009) Parallelized prediction error estimation evaluation high-dimensional models, Bioinformatics, Vol. 25(6), 827-829. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/c060-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"c060: Extended Inference for Lasso and Elastic-Net Regularized Cox and Generalized Linear Models — c060-package","text":"Thomas Hielscher t.hielscher@dkfz.de, Manuela Zucknick, Natalia Becker, Frédéric Bertrand.","code":""},{"path":"https://fbertran.github.io/c060/reference/coef.sum.intsearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Get coefficients for a model — coef.sum.intsearch","title":"Get coefficients for a model — coef.sum.intsearch","text":"Get coefficients model applying interval search tuning parameters","code":""},{"path":"https://fbertran.github.io/c060/reference/coef.sum.intsearch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get coefficients for a model — coef.sum.intsearch","text":"","code":"# S3 method for class 'sum.intsearch' coef(object, ...)"},{"path":"https://fbertran.github.io/c060/reference/coef.sum.intsearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get coefficients for a model — coef.sum.intsearch","text":"object object returned function summary.intsearch. ... additional argument(s)","code":""},{"path":"https://fbertran.github.io/c060/reference/coef.sum.intsearch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get coefficients for a model — coef.sum.intsearch","text":"named vector non-zero coeficients optimal lambda","code":""},{"path":"https://fbertran.github.io/c060/reference/coef.sum.intsearch.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get coefficients for a model — coef.sum.intsearch","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/coef.sum.intsearch.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get coefficients for a model — coef.sum.intsearch","text":"Natalia Becker \\ natalia.becker@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"Determines amount shrinkage penalized regression model fitted glmnet via cross-validation, conforming calling convention required argument complexity peperr call.","code":""},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"","code":"complexity.glmnet(response, x, full.data, ...)"},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"response survival object (Surv(time, status), binary vector entries 0 1). x n*p matrix covariates. full.data data frame containing response covariates full data set. ... additional arguments passed cv.glmnet call family.","code":""},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"Scalar value giving optimal lambda.","code":""},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"Function basically wrapper cv.glmnet package glmnet. n-fold cross-validation (default n=10) performed determine optimal penalty lambda. Cox PH regression models deviance based penalized partial log-likelihood used loss function. binary endpoints loss functions available well (see type.measure). Deviance default. Calling peperr, default arguments cv.glmnet can changed passing named list containing argument args.complexity. Note penalized Cox PH (family=\"cox\") logistic regression models (family=\"binomial\") sensible prediction error evaluation package peperr.","code":""},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"Friedman, J., Hastie, T. Tibshirani, R. (2008) Regularization Paths Generalized Linear Models via Coordinate Descent, https://web.stanford.edu/~hastie/Papers/glmnet.pdfJournal Statistical Software, Vol. 33(1), 1-22 Feb 2010https://www.jstatsoft.org/v33/i01/ Simon, N., Friedman, J., Hastie, T., Tibshirani, R. (2011) Regularization Paths Cox's Proportional Hazards Model via Coordinate Descent, Journal Statistical Software, Vol. 39(5) 1-13https://www.jstatsoft.org/v39/i05/ Porzelius, C., Binder, H., Schumacher, M. (2009) Parallelized prediction error estimation evaluation high-dimensional models, Bioinformatics, Vol. 25(6), 827-829. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/complexity.glmnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Interface for determination of penalty lambda in penalized regression model via cross-validation — complexity.glmnet","text":"Thomas Hielscher t.hielscher@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"Interface fitting penalized regression models binary survival endpoint using glmnet, conforming requirements argument fit.fun peperr call.","code":""},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"","code":"fit.glmnet(response, x, cplx, ...)"},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"response survival object (Surv(time, status), binary vector entries 0 1). x n*p matrix covariates. cplx lambda penalty value. ... additional arguments passed glmnet call family.","code":""},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"glmnet object","code":""},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"Function basically wrapper glmnet package glmnet. Note penalized Cox PH (family=\"cox\") logistic regression models (family=\"binomial\") sensible prediction error evaluation package peperr.","code":""},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"Friedman, J., Hastie, T. Tibshirani, R. (2008) Regularization Paths Generalized Linear Models via Coordinate Descent, https://web.stanford.edu/~hastie/Papers/glmnet.pdfJournal Statistical Software, Vol. 33(1), 1-22 Feb 2010https://www.jstatsoft.org/v33/i01/ Simon, N., Friedman, J., Hastie, T., Tibshirani, R. (2011) Regularization Paths Cox's Proportional Hazards Model via Coordinate Descent, Journal Statistical Software, Vol. 39(5) 1-13https://www.jstatsoft.org/v39/i05/ Porzelius, C., Binder, H., Schumacher, M. (2009) Parallelized prediction error estimation evaluation high-dimensional models, Bioinformatics, Vol. 25(6), 827-829. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/fit.glmnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Interface function for fitting a penalized regression model with glmnet — fit.glmnet","text":"Thomas Hielscher t.hielscher@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":null,"dir":"Reference","previous_headings":"","what":"function to plot a stability path — plot.stabpath","title":"function to plot a stability path — plot.stabpath","text":"Given desired family-wise error rate (FWER) stability path calculated stability.path function selects stable set features plots stability path corresponding regularization path.","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function to plot a stability path — plot.stabpath","text":"","code":"# S3 method for class 'stabpath' plot(   x,   error = 0.05,   type = c(\"pfer\", \"pcer\"),   pi_thr = 0.6,   xvar = c(\"lambda\", \"norm\", \"dev\"),   col.all = \"black\",   col.sel = \"red\",   ... )"},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function to plot a stability path — plot.stabpath","text":"x object class \"stabpath\" returned function stabpath. error desired type error level w.r.t. chosen type error rate. type type error rate used controlling number falsely selected variables. type=\"pfer\" per-family error rate controlled error corresponds expected number type errors. Selecting type=\"pfer\" error range 0 > error < 1 control family-wise error rate, .e. probability least one variable estimated stable set falsely selected. type=\"pcer\" per-comparison error rate controlled error corresponds expected number type errors divided number variables. pi_thr threshold used stability selection, range 0.5 > pi_thr < 1. xvar variable used xaxis, e.g. \"lambda\" selection probabilities plotted along log penalization parameters, \"norm\" along L1-norm \"dev\" along fraction explained deviance. col.color used variables estimated stable set col.sel color used variables estimated stable set ... arguments passed matplot","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"function to plot a stability path — plot.stabpath","text":"list four objects stable vector giving positions estimated stable variables lambda penalization parameter used stability selection lpos position penalization parameter regularization path error desired type error level w.r.t. chosen type error rate type type error rate","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"function to plot a stability path — plot.stabpath","text":"Meinshausen N. Buehlmann P. (2010), Stability Selection, Journal Royal Statistical Society: Series B (Statistical Methodology) Volume 72, Issue 4, pages 417-473. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"function to plot a stability path — plot.stabpath","text":"Martin Sill \\ m.sill@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.stabpath.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"function to plot a stability path — plot.stabpath","text":"","code":"if (FALSE) { # \\dontrun{ #gaussian set.seed(1234) x=matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000]%*%c(rep(2,5),rep(-2,5),rep(.1,990)) res <- stabpath(y,x,weakness=1,mc.cores=2) plot(res,error=.5,type='pfer') } # }"},{"path":"https://fbertran.github.io/c060/reference/plot.sum.intsearc.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Summary object for interval search models — plot.sum.intsearc","title":"Plot Summary object for interval search models — plot.sum.intsearc","text":"Produces plot summary object fitted interval search model. Plot 'visited' points iteration steps. start.N points initial points selected interval search starts.","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.sum.intsearc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Summary object for interval search models — plot.sum.intsearc","text":"","code":"# S3 method for class 'sum.intsearch' plot(x, type = \"summary\", startN = 21, ...)"},{"path":"https://fbertran.github.io/c060/reference/plot.sum.intsearc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Summary object for interval search models — plot.sum.intsearc","text":"x object class sum.intsearch returned function summary.intsearch. type type plot drawn, type=\"summary\" plot partial log likelihood deviance function tuning parameters \\(\\alpha\\) log \\(\\lambda\\). final solution highlighted solid red line. Alternativly, type=\"points\" draw distribution initial visited points interval search plotted chronological order. startN number initial points. Needed type=\"points\" ... additional argument(s)","code":""},{"path":"https://fbertran.github.io/c060/reference/plot.sum.intsearc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot Summary object for interval search models — plot.sum.intsearc","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/plot.sum.intsearc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Summary object for interval search models — plot.sum.intsearc","text":"Natalia Becker \\ natalia.becker@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/predictProb.coxnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","title":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","text":"Extracts predicted survival probabilities survival model fitted glmnet, providing interface required pmpec.","code":""},{"path":"https://fbertran.github.io/c060/reference/predictProb.coxnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","text":"","code":"# S3 method for class 'coxnet' predictProb(object, response, x, times, complexity, ...)"},{"path":"https://fbertran.github.io/c060/reference/predictProb.coxnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","text":"object fitted model class glmnet response two-column matrix columns named 'time' 'status'. latter binary variable, '1' indicating death, '0' indicating right censored. function Surv() package survival produces matrix x n*p matrix covariates. times vector evaluation time points. complexity lambda penalty value. ... additional arguments, currently used.","code":""},{"path":"https://fbertran.github.io/c060/reference/predictProb.coxnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","text":"Matrix probabilities evaluation time point times (columns) new observation (rows).","code":""},{"path":"https://fbertran.github.io/c060/reference/predictProb.coxnet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","text":"Friedman, J., Hastie, T. Tibshirani, R. (2008) Regularization Paths Generalized Linear Models via Coordinate Descent, https://web.stanford.edu/~hastie/Papers/glmnet.pdfJournal Statistical Software, Vol. 33(1), 1-22 Feb 2010https://www.jstatsoft.org/v33/i01/ Simon, N., Friedman, J., Hastie, T., Tibshirani, R. (2011) Regularization Paths Cox's Proportional Hazards Model via Coordinate Descent, Journal Statistical Software, Vol. 39(5) 1-13https://www.jstatsoft.org/v39/i05/ Porzelius, C., Binder, H., Schumacher, M. (2009) Parallelized prediction error estimation evaluation high-dimensional models, Bioinformatics, Vol. 25(6), 827-829. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/predictProb.coxnet.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract predicted survival probabilities from a glmnet fit — predictProb.coxnet","text":"Thomas Hielscher t.hielscher@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":null,"dir":"Reference","previous_headings":"","what":"Stability path for glmnet models — stabpath","title":"Stability path for glmnet models — stabpath","text":"function calculates stability path glmnet models, e.g. selection probabilities features along range regularization parameters.","code":""},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stability path for glmnet models — stabpath","text":"","code":"stabpath(   y,   x,   size = 0.632,   steps = 100,   weakness = 1,   mc.cores = getOption(\"mc.cores\", 2L),   ... )"},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stability path for glmnet models — stabpath","text":"y response variable. Like glment function: Quantitative family=\"gaussian\" family=\"poisson\" (non-negative counts). family=\"binomial\" either factor two levels, two-column matrix counts proportions. family=\"multinomial\", can nc>=2 level factor, matrix nc columns counts proportions. family=\"cox\", y two-column matrix columns named 'time' 'status'. latter binary variable, '1' indicating death, '0' indicating right censored. function Surv() package survival produces matrix x input matrix. Like glmnet function: dimension nobs x nvars; row observation vector. Can sparse matrix format (inherit class \"sparseMatrix\" package Matrix; yet available family=\"cox\") size proportion samples drawn every subsample used stability selection. steps number subsamples used stability selection. weakness weakness parameter used randomised lasso described Meinshausen B\\\"uhlmann (2010).  subsample features reweighted random weight uniformly sampled [weakness,1]. additional randomisation leads consistent estimation stable set features. mc.cores number cores used parallelization. unix like system parallelization done forking using function mclapply. windows systems socket cluster used. ... arguments passed glmnet function.","code":""},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stability path for glmnet models — stabpath","text":"object class \"stabpath\", list three objects fit fit object class \"glmnet\" returned glmnet function applied complete data set. stabpath matrix represents stability path. qs vector holding values average number non-zero coefficients w.r.t lambdas regularization path.","code":""},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Stability path for glmnet models — stabpath","text":"Meinshausen N. B\\\"uhlmann P. (2010), Stability Selection, Journal Royal Statistical Society: Series B (Statistical Methodology) Volume 72, Issue 4, pages 417–473. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Stability path for glmnet models — stabpath","text":"Martin Sill m.sill@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/stabpath.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stability path for glmnet models — stabpath","text":"","code":"if (FALSE) { # \\dontrun{ #gaussian set.seed(1234) x <- matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000]%*% c(rep(2,5),rep(-2,5),rep(.1,990)) res <- stabpath(y,x,weakness=1,mc.cores=2) plot(res)  #binomial y=sample(1:2,100,replace=TRUE) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"binomial\") plot(res)      #multinomial y=sample(1:4,100,replace=TRUE) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"multinomial\") plot(res)      #poisson N=100; p=1000 nzc=5 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) f = x[,seq(nzc)]%*%beta mu=exp(f) y=rpois(N,mu) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"poisson\") plot(res)  #Cox library(survival) set.seed(10101) N=100;p=1000 nzc=p/3 x=matrix(rnorm(N*p),N,p) beta=rnorm(nzc) fx=x[,seq(nzc)]%*%beta/3 hx=exp(fx) ty=rexp(N,hx) tcens=rbinom(n=N,prob=.3,size=1) y=cbind(time=ty,status=1-tcens) res <- stabpath(y,x,weakness=1,mc.cores=2,family=\"cox\") plot(res) } # }"},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":null,"dir":"Reference","previous_headings":"","what":"function to estimate a stable set of variables — stabsel","title":"function to estimate a stable set of variables — stabsel","text":"Given desired type error rate stability path calculated stability.path function selects stable set variables.","code":""},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function to estimate a stable set of variables — stabsel","text":"","code":"stabsel(x, error = 0.05, type = c(\"pfer\", \"pcer\"), pi_thr = 0.6)"},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function to estimate a stable set of variables — stabsel","text":"x object class \"stabpath\" returned function stabpath. error desired type error level w.r.t. chosen type error rate. type type error rate used controlling number falsely selected variables. type=\"pfer\" per-family error rate controlled error corresponds expected number type errors. Selecting type=\"pfer\" error range $0 > error < 1$ control family-wise error rate, .e. probability least one variable estimated stable set falsely selected. type=\"pcer\" per-comparison error rate controlled error corresponds expected number type errors divided number variables. pi_thr threshold used stability selection, range $0.5 > pi_thr < 1$.","code":""},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"function to estimate a stable set of variables — stabsel","text":"list four objects stable vector giving positions estimated stable variables lambda penalization parameter used stability selection lpos position penalization parameter regularization path error desired type error level w.r.t. chosen type error rate type type error rate","code":""},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"function to estimate a stable set of variables — stabsel","text":"Meinshausen N. B\\\"uhlmann P. (2010), Stability Selection, Journal Royal Statistical Society: Series B (Statistical Methodology) Volume 72, Issue 4, pages 417–473. Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"function to estimate a stable set of variables — stabsel","text":"Martin Sill \\ m.sill@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/stabsel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"function to estimate a stable set of variables — stabsel","text":"","code":"if (FALSE) { # \\dontrun{ #gaussian set.seed(1234) x=matrix(rnorm(100*1000,0,1),100,1000) y <- x[1:100,1:1000]%*%c(rep(2,5),rep(-2,5),rep(.1,990)) res <- stabpath(y,x,weakness=1,mc.cores=2) stabsel(res,error=0.05,type=\"pfer\") } # }"},{"path":"https://fbertran.github.io/c060/reference/summary.intsearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for interval search models — summary.intsearch","title":"Summary method for interval search models — summary.intsearch","text":"Produces summary fitted interval search model","code":""},{"path":"https://fbertran.github.io/c060/reference/summary.intsearch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for interval search models — summary.intsearch","text":"","code":"# S3 method for class 'intsearch' summary(   object,   digits = max(3, getOption(\"digits\") - 3),   verbose = TRUE,   first.n = 5,   ... )"},{"path":"https://fbertran.github.io/c060/reference/summary.intsearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for interval search models — summary.intsearch","text":"object object class intsearch returned function EPSGO. digits digits comma verbose default set TRUE. first.n show first.n entries , default 5. ... additional argument(s)","code":""},{"path":"https://fbertran.github.io/c060/reference/summary.intsearch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary method for interval search models — summary.intsearch","text":"list following elements info data frame four objects optimal models alpha - vector alphas lambda - vector penalization parameter lambda deviances - vector deviances n.features -vector number features selected optimal model  opt.alpha optimal value alpha opt.lambda optimal value lambda opt.error optimal value error, hier minimal diviance opt.models list optimal models optimal error","code":""},{"path":"https://fbertran.github.io/c060/reference/summary.intsearch.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Summary method for interval search models — summary.intsearch","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/summary.intsearch.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summary method for interval search models — summary.intsearch","text":"Natalia Becker \\ natalia.becker@dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/reference/tune.glmnet.interval.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for glmnet objects. — tune.glmnet.interval","title":"Wrapper function for glmnet objects. — tune.glmnet.interval","text":"Wrapper function glmnet objects used EPSGO function. function mainly used within function EPSGO","code":""},{"path":"https://fbertran.github.io/c060/reference/tune.glmnet.interval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for glmnet objects. — tune.glmnet.interval","text":"","code":"tune.glmnet.interval(   parms,   x,   y,   weights,   offset = NULL,   lambda = NULL,   type.measure = c(\"mse\", \"deviance\", \"class\", \"auc\", \"mae\"),   seed = 12345,   nfolds = 10,   foldid = NULL,   grouped = TRUE,   type.min = c(\"lambda.min\", \"lambda.1se\"),   family,   verbose = FALSE,   ... )"},{"path":"https://fbertran.github.io/c060/reference/tune.glmnet.interval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for glmnet objects. — tune.glmnet.interval","text":"parms tuning parameter alpha glmnet object x, y x matrix row refers sample column refers gene; y factor includes class sample weights observation weights. Can total counts responses proportion matrices. Default 1 observation offset vector length nobs included linear predictor (nobs x nc matrix \"multinomial\" family). Useful \"poisson\" family (e.g. log exposure time), refining model starting current fit. Default NULL. supplied, values must also supplied predict function. lambda user supplied lambda sequence. Typical usage program compute lambda sequence based nlambda lambda.min.ratio. Supplying value lambda overrides . WARNING: use care. supply single value lambda (predictions CV use predict() instead). Supply instead decreasing sequence lambda values. glmnet relies warms starts speed, often faster fit whole path compute single fit. type.measure loss use cross-validation. Currently five options, available models. default type.measure=\"deviance\", uses squared-error gaussian models (.k.type.measure=\"mse\" ), deviance logistic poisson regression, partial-likelihood Cox model. type.measure=\"class\" applies binomial multinomial logistic regression , gives misclassification error. type.measure=\"auc\" two-class logistic regression , gives area ROC curve. type.measure=\"mse\" type.measure=\"mae\" (mean absolute error) can used models except \"cox\"; measure deviation fitted mean response. seed seed nfolds number cross-validation's folds, default 10. foldid optional vector values 1 nfold identifying fold observation . supplied, nfold can missing. grouped experimental argument, default TRUE, can ignored users. models except \"cox\", refers computing nfolds separate statistics, using mean estimated standard error describe CV curve. grouped=FALSE, error matrix built observation level predictions nfold fits, summarized (apply type.measure=\"auc\"). \"cox\" family, grouped=TRUE obtains CV partial likelihood Kth fold subtraction; subtracting log partial likelihood evaluated full dataset evaluated (K-1)/K dataset. makes efficient use risk sets. grouped=FALSE log partial likelihood computed Kth fold type.min parameter chosing optimal model: 'lambda.min'- value lambda gives minimum mean cross-validated error (cvm).  'lambda.1se' - largest value lambda error within one standard error minimum. family family model, .e. cox, glm,... verbose verbose ... parameters","code":""},{"path":"https://fbertran.github.io/c060/reference/tune.glmnet.interval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for glmnet objects. — tune.glmnet.interval","text":"q.val minimal value Q.func interval defined bounds. , q.val minimum mean cross-validate d error (cvm) model model list alpha - optimal alpha lambda - optimal lambda nfolds - cross-validation's folds cvreg - cv.glmnet object optimal alpha fit - glmnet object optimal alpha optimal lambda","code":""},{"path":"https://fbertran.github.io/c060/reference/tune.glmnet.interval.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wrapper function for glmnet objects. — tune.glmnet.interval","text":"Sill M., Hielscher T., Becker N. Zucknick M. (2014), c060: Extended Inference Lasso Elastic-Net Regularized Cox Generalized Linear Models, Journal Statistical Software, Volume 62(5), pages 1–22. https://doi.org/10.18637/jss.v062.i05.","code":""},{"path":[]},{"path":"https://fbertran.github.io/c060/reference/tune.glmnet.interval.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Wrapper function for glmnet objects. — tune.glmnet.interval","text":"Natalia Becker natalia.becker dkfz.de","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-04-0","dir":"Changelog","previous_headings":"","what":"c060 0.4-0","title":"c060 0.4-0","text":"Maintainer address updated Added unit tests Turned Rd files Roxygen section .R files Used Roxygen create namespace Added package doi","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-03-1","dir":"Changelog","previous_headings":"","what":"c060 0.3-1","title":"c060 0.3-1","text":"CRAN release: 2025-08-18 Code update get rid checks notes r-devel","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-03-0","dir":"Changelog","previous_headings":"","what":"c060 0.3-0","title":"c060 0.3-0","text":"CRAN release: 2023-03-23 Code update get rid checks notes r-devel","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-02-9","dir":"Changelog","previous_headings":"","what":"c060 0.2-9","title":"c060 0.2-9","text":"CRAN release: 2022-03-03 Code update","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-02-8","dir":"Changelog","previous_headings":"","what":"c060 0.2-8","title":"c060 0.2-8","text":"CRAN release: 2021-10-06 recommended, updated links JSS.","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-02-7","dir":"Changelog","previous_headings":"","what":"c060 0.2-7","title":"c060 0.2-7","text":"CRAN release: 2021-03-16 Added github actions, pkgdown site, readme.Rmd package logo. Added NEWS.md file track changes package.","code":""},{"path":"https://fbertran.github.io/c060/news/index.html","id":"c060-02-6","dir":"Changelog","previous_headings":"","what":"c060 0.2-6","title":"c060 0.2-6","text":"CRAN release: 2021-03-01 Package archived leading maintainer change.","code":""}]
